from TreeNode import TreeNode
import numpy as np
import pandas as pd
    
class DecisionTree():
    def __init__(self):
        """
        Class that implements the tree-growing algorithm. 
        """
        self.root = None

    def tree_grow(self, x, y, nmin, minleaf, nfeat):
        """
        Main function to grow a classification tree by recursively splitting the nodes. Based on the stopping
        criteria, the tree either finds a best_split again, or terminates.

        Args: 
            x: Data Matrix (Features).
            y: Class labels (Binary).
            nmin: The number of observations that a node must contain at least, for it to be allowed to be split. 
            minleaf: The minimum number of observations required for a leaf node. 
            nfeat: The number of features that should be considered for each split. 

        Returns:
            A TreeNode that represents the root of the tree.
        """

        # Check stopping criteria (nmin or if all class labels are the same)
        if len(y) < nmin or np.all(y == y[0]):
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)

        # Find the best split and its respective value on nfeat features
        gain, split_feature, split_value, = self._best_split(x, y, nmin, minleaf, nfeat)
        
        # If no split is possible that improves the gain, return a leaf node
        if gain == 0: 
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)

        # Otherwise, create the left and right children recursively based on best feature and threshold
        left_split = x[:, split_feature] < split_value
        right_split = ~left_split
        
        # Avoid empty nodes by creating a leaf node
        if np.sum(left_split) == 0 or np.sum(right_split) == 0:
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)
        
        # Recursively grow the left and right subtrees
        left_node = self.tree_grow(x[left_split], y[left_split], nmin, minleaf, nfeat )
        right_node = self.tree_grow(x[right_split], y[right_split], nmin, minleaf, nfeat)

        # Return the root node with split information and its left and right subtrees attached
        node = TreeNode(gini=self._gini(y), gain=gain, split_feature=split_feature, split_value=split_value)
        node.left = left_node
        node.right = right_node
        return node

    def tree_pred(self, x, tr):
        """
        Predict class labels for a given set of cases using a trained decision tree.
        
        Args:
            x: A data matrix (2D array) containing attribute values of the cases.
            tr: The decision tree (TreeNode object) generated by tree_grow.
            
        Returns:
            y: A 1D array with the predicted class labels for each case in x. 
        """
        # Generate empty output array for the predictions
        y = np.zeros(x.shape[0], dtype=int)

        # Iterate over each case in x
        for i in range(x.shape[0]):
            case = x[i]
            node = tr

            # Traverse tree to find the correct leaf node
            while not node.is_leaf:
                if case[node.split_feature] < node.split_value:
                    node = node.left
                else:
                    node = node.right

            # Assign the class label of the leaf node to the prediction
            y[i] = node.c_label

        return y
    
    def tree_grow_b(self, x, y, m, nmin, minleaf, nfeat):
        """
        Grow a random forest of 'm' trees using bootstrap sampling.
        
        Args:
            x: Data Matrix (Features).
            y: Class Labels (Binary).
            m: The number of trees to grow in the forest.
            nmin: The number of observations that a node must contain at least, for it to be allowed to be split.
            minleaf: The minimum number of observations required for a leaf node.
            nfeat: The number of features that should be considered for each split.
            
        Returns:
            A list of trained trees.
        """
        trees = []
   
        for i in range(m):
            # Generate a bootstrap sample
            indices = np.random.choice(len(y), len(y))
            x_bootstrap = x[indices]
            y_bootstrap = y[indices]
        
            # Grow a tree on the bootstrap sample and add to the list of trees
            tree = self.tree_grow(x_bootstrap, y_bootstrap, nmin, minleaf, nfeat)
            trees.append(tree)
            
        return trees
    
    def tree_pred_b(self, x, trees):
        """
        Predict class labels for a given set of cases using the majority vote of a forest

        Args:
            x: A data matrix (2D array) containing attribute values of the cases.
            trees: A list of decision trees (TreeNode objects) generated by tree_grow_b.
            
        Returns:
            final_predictions: A 1D array with the predicted class labels for each case in x. 
        """
        # Generate empty output array for the predictions
        y = np.zeros((x.shape[0], len(trees)), dtype=int)

        # Get predictions from each tree
        for i, tree in enumerate(trees):
            y[:, i] = self.tree_pred(x, tree)
        
        # Make a dataframe of the predictions and check what the majority vote is
        df_y = pd.DataFrame(y) 
        final_predictions = df_y.mode(axis=1)[0].values

        return final_predictions

        
    def _gini(self, y):
        """
        Compute the Gini index for the given labels (y). 
        
        Args: 
            y: Array of binary class labels.
        
        Returns: 
            Gini index value. 
        """
        # Check for empty node
        n = len(y)
        if n == 0:
            return 0
        
        # Assign probabilities
        p1 = np.sum(y == 1) / n
        p0 = 1 - p1
        return 1 - (p1 ** 2 + p0 ** 2)

    def _best_split(self, x, y, nmin, minleaf, nfeat):
        """
        Find the best feature and threshold to split on.
        
        Args:
            x: Data Matrix (Features).
            y: Class Labels (Binary).
            nmin: The number of observations that a node must contain at least, for it to be allowed to be split.
            minleaf: The minimum number of observations required for a leaf node. 
            nfeat: The number of features that should be considered for each split. 
        
        Returns:
            best_feature: Index of the best feature to split on.
            best_threshold: Threshold value for the split.
            best_gain: The Gini gain from the split. 
        """

        m, n = x.shape
        # Check if the node has fewer observations than nmin, return no split
        if m < nmin: 
            return None, None, 0
        
        # Initialize variables
        best_gain = 0
        best_feature = None
        best_threshold = None
        current_gini = self._gini(y)

        # Randomly select nfeat features to consider for the split (depending on type of classification)
        features = np.random.choice(n, nfeat, replace=False)

        # Find the best split among the selected features
        for feature in features:
            thresholds = np.unique(x[:, feature])

            # Loop over each threshold value to evaluate the split
            for threshold in thresholds:
                # Select the feature column values
                feature_column = x[:, feature]
                filtered_values = feature_column[feature_column < threshold]

                # Check if filtered_values is not empty
                if filtered_values.size == 0:
                    # if empty skip this threshold
                    continue
                
                # Select the highest feature value below threshold
                max_value = np.max(filtered_values)
                # Update the threshold value to be the exact middle between max_value and threshold
                threshold = (threshold + max_value) / 2
  
                # Split the data based on the current threshold
                left_split = x[:, feature] < threshold
                right_split = ~left_split

                # Check both children nodes have at least minleaf observations
                if np.sum(left_split) < minleaf or np.sum(right_split) < minleaf:
                    continue # Don't allow splits that don't satisfy minleaf

                # Calculate Gini for the left and right splits
                gini_left = self._gini(y[left_split])
                gini_right = self._gini(y[right_split])

                # Weighted Gini for the split
                weighted_gini = (np.sum(left_split) * gini_left + np.sum(right_split) * gini_right) / m

                # Calculate Gini gain
                gain = current_gini - weighted_gini

                # Update best split if better one is found
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
        
        return best_gain, best_feature, best_threshold

