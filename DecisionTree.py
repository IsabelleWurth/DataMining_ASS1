from TreeNode import TreeNode
import numpy as np
import pandas as pd
    
class DecisionTree():
    def __init__(self) -> None:
        """
        Class that implements the tree-growing algorithm. 
        """
        self.root = None

    def tree_grow(self, x, y, nmin, minleaf, nfeat):
        """
        Grow a classification tree by recursively splitting the nodes. 

        Args: 
            x: Data Matrix (Features).
            y: Class labels (binary).
            nmin: The number of observations that a node must contain at least, for it to be allowed to be split. 
            minleaf: The minimum number of observations required for a leaf node. 
            nfeat: The number of features that should be considered for each split. 

        Returns:
            A TreeNode that represents the root of the tree.
        """
        # Print de huidige dataset en labels voor elke node
        #print(f"Current node dataset (size: {len(y)}):\n{x}")
        #print(f"Current node labels:\n{y}")

        # Check stopping criteria (nmin, minleaf)
        if len(y) < nmin or np.all( y == y[0]):
            # Create a lead node if number of observations is less than nmin, or if all class labels are the same
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)

        # Find the best split on nfeat features
        gain, split_feature, split_value, = self._best_split(x, y, nmin, minleaf, nfeat)

        # Print the split details
        # print(f"Best split: feature {split_feature} with threshold {split_value} (gain: {gain})")
        
        # If no split is possible, return a leaf node
        if gain == 0: 
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)

        # Otherwise, create the left and right children recursively based on best feature and threshold
        left_split = x[:, split_feature] < split_value
        right_split = ~left_split
        
        # Check whether left and right split contain data
        #print(f"Left split size: {np.sum(left_split)}, Right split size: {np.sum(right_split)}")
        
        # Recursively continue growing left and right branch
        if np.sum(left_split) == 0 or np.sum(right_split) == 0:
            # Create a leaf node
            majority_class = np.argmax(np.bincount(y))
            #print(f"One split is empty, creating leaf node with class: {majority_class}")
            return TreeNode(gini=self._gini(y), c_label=majority_class)
        
        # Recursively grow the left and right subtrees
        left_node = self.tree_grow(x[left_split], y[left_split], nmin, minleaf, nfeat )
        right_node = self.tree_grow(x[right_split], y[right_split], nmin, minleaf, nfeat)

        # Return the node with split information
        #print(f"Node with split on feature {split_feature} and threshold {split_value}")
        node = TreeNode(gini=self._gini(y), gain=gain, split_feature=split_feature, split_value=split_value)
        node.left = left_node
        node.right = right_node
        return node

    def tree_pred(self, x, tr):
        """
        Predict class labels for a given set of cases using a trained decision tree.
        
        Args:
            x: A data matrix (2D array) containing attribute values of the cases.
            tr: The decision tree (TreeNode object) generated by tree_grow (fit function).
            
        Returns:
            y: A 1D array with the predicted class labels for each case in x. 
        """
        # Output array for the predictions
        y = np.zeros(x.shape[0], dtype=int)

        # Iterate over each case in x
        for i in range(x.shape[0]):
            case = x[i]
            node = tr

            # Traverse tree to find the leaf node
            while not node.is_leaf:
                if case[node.split_feature] < node.split_value:
                    node = node.left
                else:
                    node = node.right

            # Assign the class label of the leaf node to the prediction
            y[i] = node.c_label

        return y
    
    def tree_grow_b(self, x, y, m, nmin, minleaf, nfeat):
        """
        Grow a random forest of 'm' trees using bootstrap sampling.
        
        Args:
            x: Data Matrix (Features).
            y: Class Labels (Binary).
            m: The number of trees to grow in the forest.
            nmin: The number of observations that a node must contain at least, for it to be allowed to be split.
            minleaf: The minimum number of observations required for a leaf node.
            nfeat: The number of features that should be considered for each split.
            
        Returns:
            A list of trained trees.
        """
        trees = []
            
        for i in range(m):
            # Generate a bootstrap sample
            indices = np.random.choice(len(y), len(y))
            x_bootstrap = x[indices]
            y_bootstrap = y[indices]
        
            # Grow a tree on the bootstrap sample
            tree = self.tree_grow(x_bootstrap, y_bootstrap, nmin, minleaf, nfeat)
            trees.append(tree)
            
        return trees
    
    def tree_pred_b(self, x, trees):
        y = np.zeros((x.shape[0], len(trees)), dtype=int)

        # Get predictions from each tree
        for i, tree in enumerate(trees):
            y[:, i] = self.tree_pred(x, tree)
        
        # Makes dataframe of values and checks what is the majority vote
        df_y = pd.DataFrame(y) 
        final_predictions = df_y.mode(axis=1)[0].values

        return final_predictions

        
    def _gini(self, y):
        """
        Compute the Gini index for the given labels (y). 
        
        Args: 
            y: Array of binary class labels.
        
        Returns: 
            Gini index value. 
        """
        n = len(y)
        if n == 0:
            return 0
        # Assign probabilities
        p1 = np.sum(y == 1) / n
        p0 = 1 - p1
        return 1 - (p1 ** 2 + p0 ** 2)

    def _best_split(self, x, y, nmin, minleaf, nfeat):
        """
        Find the best feature and threshold to split on.
        
        Args:
            x: Data Matrix (Features).
            y: Class Labels (Binary).
            minleaf: The minimum number of observations required for a leaf node. 
            nfeat: The number of features that should be considered for each split. 
        
        Returns:
            best_feature: Index of the best feature to split on.
            best_threshold: Threshold value for the split.
            best_gain: The Gini gain from the split. 
        """
        m, n = x.shape
     

        # Check if the node has fewer observations than mnin, return no split
        if m < nmin: 
            return None, None, 0
        
        best_gain = 0
        best_feature = None
        best_threshold = None
        current_gini = self._gini(y)

        # Randomly select nfeat features to consider for the split
        features = np.random.choice(n, nfeat, replace=False)

        # Find the best split among the selected features
        for feature in features:
            thresholds = np.unique(x[:, feature]) # Unique threshold values from features

            # Loop over each threshold value to evaluate the split
            for threshold in thresholds:
                # Select the feature column values
                feature_column = x[:, feature]
                filtered_values = feature_column[feature_column < threshold]

                # Check if filtered_values is not empty
                if filtered_values.size == 0:
                    # if empty skip this threshold
                    continue
                
                # Select the highest feature value below threshold
                max_value = np.max(filtered_values)
                # Update the threshold value to be the exact middle between max_value and threshold
                threshold = (threshold + max_value) / 2
                #print(f"Thresholdvalue:", threshold)
                # Split the data based on the current threshold
                left_split = x[:, feature] < threshold
                right_split = ~left_split

                # Check both children nodes have at least minleaf observations
                if np.sum(left_split) < minleaf or np.sum(right_split) < minleaf:
                    continue # Don't allow splits that don't satisfy minleaf

                # Calculate Gini for the left and right splits
                gini_left = self._gini(y[left_split])
                gini_right = self._gini(y[right_split])

                # Weighted Gini for the split
                weighted_gini = (np.sum(left_split) * gini_left + np.sum(right_split) * gini_right) / m

                # Calculate Gini gain
                gain = current_gini - weighted_gini

                # Update best split if better one is found
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
        
        return best_gain, best_feature, best_threshold

