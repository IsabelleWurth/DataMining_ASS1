"""
    Created by: 
        - Renske Bos: 2483947
        - Juul Petit: 6446981
        - Vincent Huf: 4546202
        - Isabelle Wurth: 6358500
"""

import pandas as pd
import numpy as np
from DecisionTree import DecisionTree
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score

class DecisionTree():
    def __init__(self):
        """
        Class that implements the tree-growing algorithm. 
        """
        self.root = None

    def tree_grow(self, x, y, nmin, minleaf, nfeat):
        """
        Main function to grow a classification tree by recursively splitting the nodes. Based on the stopping
        criteria, the tree either finds a best_split again, or terminates.

        Args: 
            x (array): Data Matrix (Features).
            y (array): Class labels (Binary).
            nmin (int): The number of observations that a node must contain at least, for it to be allowed to be split. 
            minleaf (int): The minimum number of observations required for a leaf node. 
            nfeat (int): The number of features that should be considered for each split. 

        Returns:
            A TreeNode that represents the root of the tree.
        """

        # Check stopping criteria (nmin or if all class labels are the same)
        if len(y) < nmin or np.all(y == y[0]):
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)

        # Find the best split and its respective value on nfeat features
        gain, split_feature, split_value, = self._best_split(x, y, nmin, minleaf, nfeat)
        
        # If no split is possible that improves the gain, return a leaf node
        if gain == 0: 
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)

        # Otherwise, create the left and right children recursively based on best feature and threshold
        left_split = x[:, split_feature] < split_value
        right_split = ~left_split
        
        # Avoid empty nodes by creating a leaf node
        if np.sum(left_split) == 0 or np.sum(right_split) == 0:
            majority_class = np.argmax(np.bincount(y))
            return TreeNode(gini=self._gini(y), c_label=majority_class)
        
        # Recursively grow the left and right subtrees
        left_node = self.tree_grow(x[left_split], y[left_split], nmin, minleaf, nfeat )
        right_node = self.tree_grow(x[right_split], y[right_split], nmin, minleaf, nfeat)

        # Return the root node with split information and its left and right subtrees attached
        node = TreeNode(gini=self._gini(y), gain=gain, split_feature=split_feature, split_value=split_value)
        node.left = left_node
        node.right = right_node
        return node

    def tree_pred(self, x, tr):
        """
        Predict class labels for a given set of cases using a trained decision tree.
        
        Args:
            x: A data matrix (2D array) containing attribute values of the cases.
            tr: The decision tree (TreeNode object) generated by tree_grow.
            
        Returns:
            y: A 1D array with the predicted class labels for each case in x. 
        """
        # Generate empty output array for the predictions
        y = np.zeros(x.shape[0], dtype=int)

        # Iterate over each case in x
        for i in range(x.shape[0]):
            case = x[i]
            node = tr

            # Traverse tree to find the correct leaf node
            while not node.is_leaf:
                if case[node.split_feature] < node.split_value:
                    node = node.left
                else:
                    node = node.right

            # Assign the class label of the leaf node to the prediction
            y[i] = node.c_label

        return y
    
    def tree_grow_b(self, x, y, m, nmin, minleaf, nfeat):
        """
        Grow a random forest of 'm' trees using bootstrap sampling.
        
        Args:
            x (array): Data Matrix (Features).
            y (array): Class Labels (Binary).
            m (int): The number of trees to grow in the forest.
            nmin (int): The number of observations that a node must contain at least, for it to be allowed to be split.
            minleaf (int): The minimum number of observations required for a leaf node.
            nfeat (int): The number of features that should be considered for each split.
            
        Returns:
            A list of trained trees.
        """
        trees = []
   
        for i in range(m):
            # Generate a bootstrap sample
            indices = np.random.choice(len(y), len(y))
            x_bootstrap = x[indices]
            y_bootstrap = y[indices]
        
            # Grow a tree on the bootstrap sample and add to the list of trees
            tree = self.tree_grow(x_bootstrap, y_bootstrap, nmin, minleaf, nfeat)
            trees.append(tree)
            
        return trees
    
    def tree_pred_b(self, x, trees):
        """
        Predict class labels for a given set of cases using the majority vote of a forest

        Args:
            x: A data matrix (2D array) containing attribute values of the cases.
            trees: A list of decision trees (TreeNode objects) generated by tree_grow_b.
            
        Returns:
            final_predictions: A 1D array with the predicted class labels for each case in x. 
        """
        # Generate empty output array for the predictions
        y = np.zeros((x.shape[0], len(trees)), dtype=int)

        # Get predictions from each tree
        for i, tree in enumerate(trees):
            y[:, i] = self.tree_pred(x, tree)
        
        # Make a dataframe of the predictions and check what the majority vote is
        df_y = pd.DataFrame(y) 
        final_predictions = df_y.mode(axis=1)[0].values

        return final_predictions

        
    def _gini(self, y):
        """
        Compute the Gini index for the given labels (y). 
        
        Args: 
            y: Array of binary class labels.
        
        Returns: 
            Gini index value. 
        """
        # Check for empty node
        n = len(y)
        if n == 0:
            return 0
        
        # Assign probabilities
        p1 = np.sum(y == 1) / n
        p0 = 1 - p1
        return 1 - (p1 ** 2 + p0 ** 2)

    def _best_split(self, x, y, nmin, minleaf, nfeat):
        """
        Find the best feature and threshold to split on.
        
        Args:
            x (array): Data Matrix (Features).
            y (array): Class Labels (Binary).
            nmin (int): The number of observations that a node must contain at least, for it to be allowed to be split.
            minleaf (int): The minimum number of observations required for a leaf node. 
            nfeat (int): The number of features that should be considered for each split. 
        
        Returns:
            best_feature: Index of the best feature to split on.
            best_threshold: Threshold value for the split.
            best_gain: The Gini gain from the split. 
        """
        
        m, n = x.shape
        # Check if the node has fewer observations than nmin, return no split
        if m < nmin: 
            return None, None, 0
        
        # Initialize variables
        best_gain = 0
        best_feature = None
        best_threshold = None
        current_gini = self._gini(y)

        # Randomly select nfeat features to consider for the split (depending on type of classification)
        features = np.random.choice(n, nfeat, replace=False)

        # Find the best split among the selected features
        for feature in features:
            thresholds = np.unique(x[:, feature])

            # Loop over each threshold value to evaluate the split
            for threshold in thresholds:
                # Select the feature column values
                feature_column = x[:, feature]
                filtered_values = feature_column[feature_column < threshold]

                # Check if filtered_values is not empty
                if filtered_values.size == 0:
                    # if empty skip this threshold
                    continue
                
                # Select the highest feature value below threshold
                max_value = np.max(filtered_values)
                # Update the threshold value to be the exact middle between max_value and threshold
                threshold = (threshold + max_value) / 2
  
                # Split the data based on the current threshold
                left_split = x[:, feature] < threshold
                right_split = ~left_split

                # Check both children nodes have at least minleaf observations
                if np.sum(left_split) < minleaf or np.sum(right_split) < minleaf:
                    continue # Don't allow splits that don't satisfy minleaf

                # Calculate Gini for the left and right splits
                gini_left = self._gini(y[left_split])
                gini_right = self._gini(y[right_split])

                # Weighted Gini for the split
                weighted_gini = (np.sum(left_split) * gini_left + np.sum(right_split) * gini_right) / m

                # Calculate Gini gain
                gain = current_gini - weighted_gini

                # Update best split if better one is found
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
        
        return best_gain, best_feature, best_threshold

class TreeNode():
    def __init__(self, gini, gain=0, c_label=None, split_feature=None, split_value=None):
        """
        Class to initialize a TreeNode. 

        Args:
            gini (float): Gini index of the node. 
            gain (float): Gini gain (impurity reduction) from the split.
            c_label (int: 0 or 1): Majority class label - only used for the leaf nodes.
            split_feature (int): Index of the feature to split on (if not leaf node)
            split_value (float): Threshold value to split on (if not lead node)
        """
        # Gini index to determine quality of the split
        self.gini = gini

        # Gini gain for the node
        self.gain = gain

        # Class label - majority class label which is only used for leaf node
        self.c_label = c_label

        # Feature index and threshold value for splitting - if node is not a leaf node
        self.split_feature = split_feature
        self.split_value = split_value

        # Left and right child nodes for splitting
        self.left = None
        self.right = None

        # Flag to indicate if node is a leaf node
        self.is_leaf = c_label is not None

    def is_terminal(self):
        """
        Checks whether node is a leaf node
        
        Returns: 
            True is node is a leaf node, False otherwise
        """
        return self.is_leaf

def eclipse(file):
    """
    Loads and processes the Eclipse dataset from a CSV file, extracting features and binary target labels.

    Args:
        file (csv): The path to the CSV file containing the Eclipse dataset.

    Returns:
        tuple: A tuple containing:
            - variables: The 41 features used as predictors.
            - target: Binary target labels, indicating the presence (1) or absence (0) of post bugs.
    """
    df = pd.read_csv(file, sep = ';')

    # 41 features as predictors and the 'post' feature to predict 
    predictors = ['pre','FOUT_avg', 'FOUT_max', 'FOUT_sum', 'MLOC_avg', 'MLOC_max', 'MLOC_sum', 'NBD_avg', 'NBD_max', 'NBD_sum', 'NOF_avg', 'NOF_max', 'NOF_sum', 'NOM_avg', 'NOM_max', 'NOM_sum', 'NSF_avg', 'NSF_max', 'NSF_sum', 'NSM_avg', 'NSM_max', 'NSM_sum', 'PAR_avg', 'PAR_max', 'PAR_sum', 'VG_avg', 'VG_max', 'VG_sum', 'NOCU', 'ACD_avg', 'ACD_max', 'ACD_sum', 'NOI_avg', 'NOI_max', 'NOI_sum', 'NOT_avg', 'NOT_max', 'NOT_sum', 'TLOC_avg', 'TLOC_max', 'TLOC_sum']
    target = 'post'
    df[target] = df[target].astype(int)

    # Change target variables such that it indicates whether post bugs are found or not (instead of how many)
    df.loc[df[target] > 0, target] = 1
    variables = df[predictors].to_numpy()
    target = df[target].to_numpy()
    return variables, target

def evaluate(real_class, predictions):
    """
    Evaluates the classification model by generating a confusion matrix from the true and predicted labels.

    Args:
        real_class (array-like int): The true class labels.
        predictions (array-like int): The predicted class labels from the model.

    Returns:
        pandas.DataFrame: A confusion matrix as a DataFrame.
    """
    # Make a confusion matrix
    cm = confusion_matrix(real_class, predictions) 
    cm_df = pd.DataFrame(cm)
    return cm_df

def metrics(y_test_labels, y_pred_labels): 
    """
    Calculates the accuracy, precision, and recall metrics for a classification model.

    Args:
        y_test_labels (arry-like int): The true class labels for the test set.
        y_pred_labels (array-like int): The predicted class labels from the model.

    Returns:
        tuple: A tuple containing:
            - accuracy: The accuracy of the model.
            - precision: The precision of the model.
            - recall: The recall of the model.
    """
    # Calculate metrics for the Decision Tree (1=pos en 0=neg)
    accuracy = accuracy_score(y_test_labels, y_pred_labels)
    precision = precision_score(y_test_labels, y_pred_labels)
    recall = recall_score(y_test_labels, y_pred_labels)

    return accuracy, precision, recall


if __name__ == "__main__":
    # Load dataset
    x_train, y_train = eclipse('eclipse_train.csv')
    x_test, y_test = eclipse('eclipse_test.csv')

    # Initiate DecisionTree
    single_tree = DecisionTree()
    bagging_tree = DecisionTree()
    rf_tree = DecisionTree()

    # Grow the decision tree (single tree, bagging and random forest)
    single_tree.root = single_tree.tree_grow(x_train, y_train, nmin=20, minleaf=5, nfeat=41)
    bagging_trees = bagging_tree.tree_grow_b(x_train, y_train, m=100, nmin=15, minleaf=5, nfeat=41)
    rf_trees = rf_tree.tree_grow_b(x_train, y_train, m=100, nmin=15, minleaf=5, nfeat=6)

    # Create predictions using the trained tree and evaluate (single tree)
    predictions_single = single_tree.tree_pred(x_test, single_tree.root)
    # Create predictions using the trained tree and evaluate (bagging)
    predictions_bagging = bagging_tree.tree_pred_b(x_test, bagging_trees)
    # Create predictions using the trained tree and evaluate (random forest)
    predictions_rf = rf_tree.tree_pred_b(x_test, rf_trees)
        
    # Calculate metrics for this run - TEST SINGLE 
    single_tree_metrics_run = metrics(y_test, predictions_single)
    single_tree_conf_matrix = evaluate(y_test, predictions_single)
        
    # Calculate metrics for this run - TEST BAGGING
    bagging_metrics_run = metrics(y_test, predictions_bagging)
    bagging_conf_matrix = evaluate(y_test, predictions_bagging)

    # Calculate metrics for this run - TEST RANDOM FOREST
    rf_metrics_run = metrics(y_test, predictions_rf)
    rf_conf_matrix = evaluate(y_test, predictions_rf)

    # Summarize and print the results for each model
    print("\nMetrics for Single Tree:")
    print(f"Accuracy = {single_tree_metrics_run[0]}, Precision = {single_tree_metrics_run[1]}, Recall = {single_tree_metrics_run[2]}")
    
    print("\nMetrics for Bagging:")
    print(f"Accuracy = {bagging_metrics_run[0]}, Precision = {bagging_metrics_run[1]}, Recall = {bagging_metrics_run[2]}")

    print("\nMetrics for Random Forest:")
    print(f"Accuracy = {rf_metrics_run[0]}, Precision = {rf_metrics_run[1]}, Recall = {rf_metrics_run[2]}")

    # Print the confusion matrices if needed
    print(f"Confusion Matrix for Single Tree Run \n{single_tree_conf_matrix}")
    print(f"Confusion Matrix for Bagging: \n{bagging_conf_matrix}")
    print(f"Confusion Matrix for Random Forest: \n{rf_conf_matrix}")